{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a9e579-3d53-48ce-a4b5-cb8a387590ab",
   "metadata": {},
   "source": [
    "## Python Test case taken from MBPP dataset. \n",
    "- test is to write a function that gives the perimeter of the square.\n",
    "- i did not load the dataset from hugging face. Simply downloaded it, picked a sample python test case manually and pasted it here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62820478-9edf-4dd2-a9eb-19d217be92ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = {\n",
    "    \"task_id\": 17,\n",
    "    \"function_name\": \"square_perimeter\",\n",
    "    \"signature\": \"def square_perimeter(a):\",\n",
    "    \"prompt\": \"Write a function to find the perimeter of a square.\",\n",
    "    \"asserts\": [\n",
    "        \"assert square_perimeter(10) == 40\",\n",
    "        \"assert square_perimeter(5) == 20\",\n",
    "        \"assert square_perimeter(4) == 16\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bd79c-60fb-495f-b0d0-a408ce7388d8",
   "metadata": {},
   "source": [
    "## Creating a folder Structure. nothing else!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f37a26-dda3-4e85-9689-d0ec51fc9c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete for Task 17 – square_perimeter\n",
      "Folders created:\n",
      "/home/jovyan/PyTest_Scenarios-2/solutions\n",
      "/home/jovyan/PyTest_Scenarios-2/tests\n",
      "/home/jovyan/PyTest_Scenarios-2/logs\n"
     ]
    }
   ],
   "source": [
    "# folder structure \n",
    "import os, json, pathlib\n",
    "\n",
    "ROOT = pathlib.Path(\".\").resolve()\n",
    "SOL_DIR = ROOT / \"solutions\"\n",
    "TEST_DIR = ROOT / \"tests\"\n",
    "LOG_DIR  = ROOT / \"logs\"\n",
    "\n",
    "for d in (SOL_DIR, TEST_DIR, LOG_DIR):\n",
    "    d.mkdir(exist_ok=True)\n",
    "\n",
    "# Save task for reference\n",
    "with open(LOG_DIR / f\"task_{task['task_id']}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(task, f, indent=2)\n",
    "\n",
    "print(f\"Setup complete for Task {task['task_id']} – {task['function_name']}\")\n",
    "print(\"Folders created:\", SOL_DIR, TEST_DIR, LOG_DIR, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fddb41-3250-45a7-b82b-4f16907b5f21",
   "metadata": {},
   "source": [
    "## Creating a Sample Pytest Reference. --> (this is like a baseline) \n",
    "- creates 2 files - solutions file and the pytest script for testing it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77860958-2858-4607-9146-2de7b3353010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote files:\n",
      "- /home/jovyan/PyTest_Scenarios-2/solutions/square_perimeter.py\n",
      "- /home/jovyan/PyTest_Scenarios-2/tests/test_square_perimeter.py\n"
     ]
    }
   ],
   "source": [
    "# Reference solution (solutions/square_perimeter.py)\n",
    "ref_code = f\"\"\"\n",
    "{task['signature']}\n",
    "    return 4 * a\n",
    "\"\"\".lstrip()\n",
    "\n",
    "sol_path = SOL_DIR / f\"{task['function_name']}.py\"\n",
    "with open(sol_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(ref_code)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Create pytest file (tests/test_square_perimeter.py)\n",
    "gt_pytest = f\"\"\"\n",
    "import pytest\n",
    "from solutions.{task['function_name']} import {task['function_name']}\n",
    "\n",
    "def test_values():\n",
    "{chr(10).join('    ' + line for line in task['asserts'])}\n",
    "\"\"\"\n",
    "\n",
    "test_path = TEST_DIR / f\"test_{task['function_name']}.py\"\n",
    "with open(test_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(gt_pytest.strip() + \"\\n\")\n",
    "\n",
    "print(\"Wrote files:\")\n",
    "print(\"-\", sol_path)\n",
    "print(\"-\", test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3174497-d556-4bfc-962a-ac456ac87c72",
   "metadata": {},
   "source": [
    "## Loading DeepSeek coder LLM from HuggingFace. No local downloads with Ollama. \n",
    "- model used - deepseek-ai/deepseek-coder-1.3b-instruct\n",
    "- How did I decide the model?\n",
    "   - decided to use codegemma--> realized it was gated, required access. I dont have much time. \n",
    "   - then chose: deepseek coder 6.7b --> spent 6 hours troubleshooting and was facing issues quantizing and loading the model. Turned out it was too big for my machine and resources available .\n",
    "   - Hence settled for the smallest LLM- deepseek coder 1.3b instruct model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a37a8b6-b71a-4d1b-9f80-a45d6a5b399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing libraries to use the LLM from Hugging face \n",
    "!pip3 install transformers accelerate torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de8d2f7d-1432-4c2b-8626-aa2c63074245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: deepseek-ai/deepseek-coder-1.3b-instruct\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Choose model name (change to full or instruct variant as you prefer)\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "# Load tokenizer & model (use 8-bit if memory is tight)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded:\", MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3d187d-dd8b-44e5-b492-f84b040988e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper ready.\n"
     ]
    }
   ],
   "source": [
    "#Helper function for prompting DeepSeek:\n",
    "\n",
    "def generate_llm_output(prompt, max_new_tokens=256, temperature=0.3):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Return only the part after the prompt\n",
    "    return text[len(prompt):].strip()\n",
    "\n",
    "print(\"Helper ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db6336a-0c52-4353-8a39-c7a67482680d",
   "metadata": {},
   "source": [
    "## Demonstration that DeepSeek wrote a pytest case with a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40624f04-b578-4768-88b5-a49e93dd3c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the test file:\n",
      "\n",
      "```python\n",
      "import pytest\n",
      "from solutions.square_perimeter import square_perimeter\n",
      "\n",
      "def test_square_perimeter():\n",
      "    assert square_perimeter(10) == 40\n",
      "    assert square_perimeter(5) == 20\n",
      "    assert square_perimeter(4) == 16\n",
      "```\n",
      "\n",
      "Please note that the `square_perimeter` function is not defined in the provided code, so you need to define it first.\n",
      "\n",
      "If you want to use a mock function, you can do so as follows:\n",
      "\n",
      "```python\n",
      "import pytest\n",
      "from unittest.mock import patch\n",
      "from solutions.square_perimeter import square_perimeter\n",
      "\n",
      "@patch('solutions.square_perimeter.square_perimeter')\n",
      "def test_square_perimeter(mock_square_perimeter):\n",
      "    mock_square_perimeter.return_value = 40\n",
      "    assert square_perimeter(10) == 40\n",
      "\n",
      "    mock_square_perimeter.return_value = 20\n",
      "    assert square_perimeter(5) == 20\n",
      "\n",
      "    mock_square\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"\"\"Write a pytest test file for a Python function `square_perimeter(a)` \n",
    "that returns the perimeter of a square.\n",
    "Import as: from solutions.square_perimeter import square_perimeter\n",
    "Include at least 3 test cases: 10→40, 5→20, 4→16.\n",
    "Output only valid Python code.\"\"\"\n",
    "generated_test = generate_llm_output(prompt)\n",
    "print(generated_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471a898a-8dff-45e2-878f-a23099cb151f",
   "metadata": {},
   "source": [
    "## these code file manually in folder - deepseek responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a18ef72-370e-472a-be3b-e83b9376f20b",
   "metadata": {},
   "source": [
    "## Evaluation: Here I am using the LLM for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2842d271-d84d-4fd7-a262-d3854bbbb1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL RAW OUTPUT ===\n",
      "Solution:\n",
      "\n",
      "```python\n",
      "import solutions.square_perimeter\n",
      "\n",
      "def test_square_perimeter():\n",
      "    assert solutions.square_perimeter.square_perimeter(10) == 40\n",
      "    assert solutions.square_perimeter.square_perimeter(5) == 20\n",
      "    assert solutions.square_perimeter.square_perimeter(4) == 16\n",
      "```\n",
      "\n",
      "Solution explanation:\n",
      "\n",
      "The function square_perimeter(a) is supposed to return the perimeter of a square. The formula for the perimeter of a square is 4 times the length of one side. So, if we input a value for a, the function will return the perimeter of the square.\n",
      "\n",
      "Test cases:\n",
      "\n",
      "1. square_perimeter(10) returns 40\n",
      "2. square_perimeter(5) returns 20\n",
      "3. square_perimeter(4) returns 16\n",
      "\n",
      "Score: 5\n",
      "Reason: The function works as expected and the test cases cover all possible inputs.\n",
      "\n",
      " Parsed Score: 5/5\n"
     ]
    }
   ],
   "source": [
    "import re, pathlib, torch\n",
    "\n",
    "MODEL_NAME = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "judge = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "#Load DeepSeek-generated pytest text \n",
    "pytest_text = pathlib.Path(\"Deepseek_Responses/DS-Pytest-Response.py\").read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Problem + checklist for evaluation\n",
    "problem_text = \"Write a function square_perimeter(a) that returns the perimeter of a square.\"\n",
    "checklist = (\n",
    "    \"1. Correct import from solutions.square_perimeter\\n\"\n",
    "    \"2. Tests at least 10→40, 5→20, 4→16\\n\"\n",
    "    \"3. Uses clean asserts, no I/O or randomness\\n\"\n",
    "    \"4. Readable and concise\"\n",
    ")\n",
    "\n",
    "# Build prompt for the local judge\n",
    "judge_prompt = f\"\"\"\n",
    "You are a strict code-review evaluator.\n",
    "\n",
    "Problem:\n",
    "{problem_text}\n",
    "\n",
    "Checklist:\n",
    "{checklist}\n",
    "\n",
    "Pytest code:\n",
    "\n",
    "Evaluate the pytest test quality on a scale of 1–5:\n",
    "- 5 = fully meets checklist\n",
    "- 3 = partially meets\n",
    "- 1 = fails\n",
    "\n",
    "Reply only in this format:\n",
    "Score: <number 1-5>\n",
    "Reason: <short reason>\n",
    "\"\"\"\n",
    "\n",
    "# Run inference\n",
    "inputs = tokenizer(judge_prompt, return_tensors=\"pt\").to(judge.device)\n",
    "with torch.no_grad():\n",
    "    output_tokens = judge.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=300,\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "decoded = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "#Display results\n",
    "response_text = decoded[len(judge_prompt):].strip()\n",
    "print(\"=== MODEL RAW OUTPUT ===\")\n",
    "print(response_text or \"(empty response)\")\n",
    "\n",
    "match = re.search(r\"Score:\\s*([1-5])\", response_text)\n",
    "if match:\n",
    "    print(f\"\\n Parsed Score: {match.group(1)}/5\")\n",
    "else:\n",
    "    print(f\"\\n Could not find a numeric score in the response.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
